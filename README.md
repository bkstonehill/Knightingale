Knightingale: A Block-Recurrent Vision Transformer Chess Engine
Braden Stonehill
bstoneh1@students.kennesaw.edu
https://github.com/bkstonehill/Knightingale
Department of Computer Science, Kennesaw State University
Marietta, GA
 
Abstract
There have been many developments for utilizing neural networks to play and evaluate chess positions such as Stockfish, Leela Zero, and many other engines that utilize various neural network architectures including CNNs and text Transformers. As Transformers have been increasingly used for vision tasks, I propose a hybrid architecture of a Block-Recurrent Transformer and a Vision Transformer for evaluating chess positions in an engine called Knightingale that trains purely through recurrence rather than the common approach of reinforcement learning. While unable to beat the current state-of-the-art engine, Stockfish, its potential to learn the game of chess can be drastically improved with sufficient computational resources and training data.
1.	Introduction
Chess is a complex game that has undergone much research in the realm of computer science due to its very high value of unique reachable positions, the approaches in tree searching algorithms to explore the game tree, and in terms of how to evaluate how good a position is for a player. Traditionally, hand-crafted heuristic functions, created based on knowledge of the game, were used to evaluate any given position. A simple heuristic function could be to count the material for each player. However, the heuristic functions still struggle against experienced players.
In recent years, many approaches for evaluating a position have been developed with the use of neural networks, such as Stockfish (Romstad T., et al. for further details), with many engine designs that utilize CNNs, and some Transformer based designs. 
Transformer models have been a recent development in neural network architecture for processing a sequence without the need for recursive architectures using attention (Vaswani, A, et al. for further details). Attention utilizes global context to determine the importance of a token in a sequence. This concept has been expanded to image-based Transformers through Vision Transformers. A Vision Transformer splits an image into patches of images, encoded positional embeddings, and process the image sequences the same as processing text data (Dosovitskiy, A. et al). As some sequences contain too many tokens to load into a computational environment at once and to allow models to work with variable length sequences, there have been many approaches by applying recurrence to Transformers. Transformer-XL was an approach to introduce recurrence by caching the hidden state generated within the network and cache the hidden state to be used on the next execution of the model (Dai, Z., et al.). Building upon the idea of adding recurrence to Transformer models, the Block-Recurrent Transformer was also introduced. The network utilizes a recurrent layer inside a traditional Transformer architecture to pass a generated hidden state to each iteration and uses cross attention in combination with self-attention to generate the output and hidden state values (Hutchins, D. et al.). The design of the network was intended to process a block of tokens within a sequence with a sliding context window that is obtained through the recurrent structure (Hutchins, D. et al.).
As chess is game that generates a sequence of moves and board positions, I propose a hybrid architecture of a Vision Transformer and Block-Recurrent Transformer, Block-Recurrent Vision Transformer (BRViT), to treat chess positions as a sequence of images in which each image is considered as a block of tokens. The architecture will be used to evaluate positions and return a metric representing the advantage a player has and is incorporated into an engine referred to as Knightingale.
2.	Related Work
There have been related works for applying recurrence to Vision Transformers such as RViT which creates recurrence through an attention gate. A single layer will combine the hidden state and input to generate the key, value, and queries for attention. The residual connections is expanded to combine the hidden state with the output of the attention gate to create the hidden state for the next iteration (Yang, J, et al.).
3.	Methodology
3.1 Search Algorithms
The chess engine required an agent to evaluate positions and determine the best move to make. A common approach to searching the game tree generated from all the possible positions is the Minimax algorithm. The algorithm picks the best move by using a depth-first search to a given position and propagating the evaluated metric back to the root node. The algorithm assumes that the game is a zero-sum game in which each player is attempting to prevent the other player from winning. As such, as the evaluated metric is propagated through the tree, at alternating steps, the algorithm attempts to maximize the score for the current player while minimizing the score for the opposing player. The Negamax framework was used in Knightingale for the search algorithm which is a variation of Minimax that increases computational efficiency.
A quiescence search algorithm was also implemented to stabilize evaluation scores for positions which may evaluate to a higher score for the current player but results in a disadvantage. The algorithm continues the Minimax search but is limited to only moves that capture pieces to ensure that a position does not result in a disadvantage. For example, the evaluation may return that a Queen taking a Pawn as advantage for the current player but results in the Queen being captured the next turn resulting in a disadvantage.
3.2 Search Improvements
As chess is a game that can generate many positions for any given board state, it can result in computationally expensive searches. To improve the performance and efficiency of the search algorithm, Alpha-Beta pruning, Transposition tables, and Iterative deepening were implemented.
Alpha-Beta pruning is an approach to minimize the number of states processed by preventing searches on states that are not better than a state that has already been evaluated. A lower and upper bound for possible values for a position is tracked when searching through the game tree and ignores branches in which the best score is outside the bounds of the best move searched so far.
Transposition tables were implemented to add dynamic programming into the engine to save results for positions within a hash table. The positions were replaced based on a combination of maximum depth the position was evaluated at and the time since last used in the search algorithm. The table itself does not reduce the computation directly by storing the metrics for a position, but rather provide a way to sort the moves based on their evaluated metrics to convert the Minimax algorithm from a depth-first search to a best-first search which results in optimal cutoffs for the Alpha-Beta framework.
Iterative deepening alters the depth-first approach to iterative search up to a given depth before increasing the depth limit. In a chess engine, this allows for a best move to be chosen every iteration allowing the engine to be limited by time and to explore as many states as possible within a set time limit rather than attempting to search every possible position. In combination with Transposition tables and the Alpha-Beta framework, it results in the search algorithm to evaluate positions at a given depth and to order the positions by value to maximize the expected cutoffs on the next iteration with an increased depth limit.
3.3 Model Architecture
The proposed model for evaluating chess positions follows the same structure as the original Transformer Encoder model (Vaswani, A. et al) but with half the number of encoder layers and number of heads for multi-head attention. It implements the preprocessing required for Vision Transformers (Dosovitskiy, A., et al.) by separating an image into flattened image patches and adding positional embeddings. The Block-Recurrent Transformer layer was recreated based upon the research for the architecture (Hutchins, D. et al) and was added as the last layer in the Transformer layers. A feed-forward network was added as a head with a single output for regression representing the advantage for a player from the range of -1 to 1 for any board position; -1 representing a complete win for black and 1 representing a complete win for white.
4.	Experiments
4.1 Dataset
The dataset was collected from Lichess open database which contains records of online games played on the Lichess platform. The database contains PGN files representing collections of games in standard PGN notation for representing chess games separated by month. The dataset used was the collection of games from March 2023 containing approximately 100 million chess games.
The data was parsed and cleaned to only include games that finished completely by stalemate or by checkmate. Due to the size and computation requirements of the dataset, approximately 200Gb of data, only a subset of 10 thousand games were able to be extracted and used for training purposes. 
The extracted data was then converted from a text of move sequences to a sequence of board positions. Each board position was represented as an image with values ranging from -6 to 6 where 6 represented the King, 5 represented the Queen, etc.
4.2 Training
As each game of chess varies in the number of moves taken before reaching the end, each sequence in a batch was padded with empty positions to ensure each sequence was of equal length. This ensured that the model could process batches of sequences simultaneously rather than one sequence at a time. The model was trained on the subset of collected games for 100 epochs using a Nesterov Adam optimizer and Mean Square Error loss function.
4.3 Testing
To test the performance of the model, the chess engine competed with the state-of-the-art engine, Stockfish, for 100 rounds alternating between playing the white and black pieces. Each engine was given 100ms to search and evaluate positions before making the calculated best move. Each game was tracked and saved in PGN format so that the games can be analyzed. After testing, Knightingale was unable to beat Stockfish or draw in any games and seemed unable to learn any strategies or tactics.
5.	Conclusions
Transformer models require a large amount of data to properly learn weights for attention. Due to time constraints and computational limitations of a single machine used for training, a large enough subset of data was unable to be obtained and used to adequately train the model which may result in the seemingly lack of strategy and tactics. Many other considerations can be accounted for the performance of the model such as the efficiency of the code structure, interpreted language Python versus compiled C++ for Stockfish, and the hyperparameters of the model: number of layers, number of nodes in the feed-forward networks inside the Transformer Encoder layers, number of heads for multi-head attention, activation function used for the output layer, placement of the recurrent layers, and number of epochs. Another consideration is that by padding the sequences of positions, the amount of additional empty positions seen during training may result in the model being unable to generalize.
For future work, I would plan to explore tuning the hyperparameters for the model and provide adequate training as well as optimizing the implementation so that more positions can be evaluated within the same allotment of time. The Block-Recurrent Vision Transformer architecture could be explored for other applications for image sequences such as video classification or expanded to process blocks of frames in a video utilizing the sliding context window throughout the sequence.
References
Hutchins, D., Schlag, I., Wu, Y., Dyer, E., & Neyshabur, B. (2022). Block-Recurrent Transformers. ArXiv. https://arxiv.org/abs/1901.02860
Yang, J., Dong, X., Liu, L., Zhang, C., Shen, J., & Yu, D. (2022). Recurring the Transformer for Video Action Recognition. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 14063–14073.
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. ArXiv. https://arxiv.org/abs/1901.02860
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. International Conference on Learning Representations. https://openreview.net/forum?id=YicbFdNTTy
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need. ArXiv. https://arxiv.org/abs/1706.03762
Romstad, T., Costalba, M., Kiiski, J., Linscott, G., Nasu, Y., Isozaki, M., Noda, H., Aditya, Petrescu, A., Jose, A., Savard, A., Feh, A., Kure, A., Pagel, A., Menezes, A., AlZhrani, A., Vetrov, A., Grant, A., Neporada, A., … Negri, V. Stockfish. https://github.com/official-stockfish/Stockfish

